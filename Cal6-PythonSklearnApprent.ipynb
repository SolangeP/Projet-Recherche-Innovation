{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "name": "",
  "signature": "sha256:a5542b24b43d5b105b6d94b0b83750757e722574f2ce7e9fd0c9101a35947eec"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center>\n",
      "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
      "\n",
      "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
      "\n",
      "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 250px; display: inline\" alt=\"IMT\"/> </a>\n",
      "</center>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "<a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 200px; display: inline\" alt=\"Python\"/></a> [pour Statistique et Science des Donn\u00e9es](https://github.com/wikistat/Intro-Python)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Apprentissage Statistique / Machine avec <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 150px; display: inline\" alt=\"Python\"/></a> & <a href=\"http://scikit-learn.org/stable/#\"><img src=\"http://scikit-learn.org/stable/_static/scikit-learn-logo-small.png\" style=\"max-width: 180px; display: inline\" alt=\"Scikit-Learn\"/></a>\n",
      "**R\u00e9sum\u00e9**: Ce calepin introduit l'utilisation de la librairie `scikit-learn` pour la mod\u00e9lisation et l'apprentissage. Pourquoi utiliser `scikit-learn` ? Ou non ? Liste des fonctionnalit\u00e9s, quelques exemples de mise en oeuvre de mod\u00e9lisation ([r\u00e9gression logistique](http://wikistat.fr/pdf/st-m-app-rlogit.pdf), [$k$-plus proches voisins](http://wikistat.fr/pdf/st-m-app-add.pdf), [arbres de d\u00e9cision](http://wikistat.fr/pdf/st-m-app-cart.pdf), [for\u00eats al\u00e9atoires](http://wikistat.fr/pdf/st-m-app-agreg.pdf). Optimisation des param\u00e8tres (complexit\u00e9) des mod\u00e8les par [validation crois\u00e9e](http://wikistat.fr/pdf/st-m-app-risque-estim.pdf). Fontions de cha\u00eenage (*pipeline*) de transformations et estimations. D'autres fonctionalit\u00e9s de `Scikit-learn` sont abord\u00e9es dans les calepins du [d\u00e9pot sur l'apprentissage](https://github.com/wikistat/Apprentissage) statistique. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 1 Introduction\n",
      "### 1.1 `Scikit-learn` *vs.*  R\n",
      "L'objectif de ce tutoriel est d'introduire l'utilisation de la librairie `scikit-learn` de Python. Seule l'utilisation directe des fonctions de mod\u00e9lisation sont abord\u00e9es d'une mani\u00e8re analogue \u00e0 la mise en oeuvre de R dont les librairies offrent l'acc\u00e8s \u00e0 bien plus de m\u00e9thodes. La comparaison avec R repose sur les remarques suivantes.\n",
      "\n",
      "- Cette librairie manipule des objets de classe `array` de `numpy` *charg\u00e9s en m\u00e9moire* et donc de taille limit\u00e9e par la RAM de l'ordinateur; de fa\u00e7on analogue R charge en RAM des objets de type `data.frame`.\n",
      "- `Scikit-learn` (0.18) ne reconna\u00eet pas (ou pas encore ?) la classe `DataFrame` de `pandas`; `scikit-learn` utilise la classe `array` de `numpy`. C'est un probl\u00e8me pour la gestion de variables qualitatives complexes. Une variable binaire est simplement remplac\u00e9e par un codage *(0,1)* mais, en pr\u00e9sence de plusieurs modalit\u00e9s, traiter celles-ci comme des entiers n'a pas de sens statistique et remplacer une variable qualitative par l'ensemble des indicatrices (*dummy variables (0,1)*) de ses modalit\u00e9s  complique les strat\u00e9gies de s\u00e9lection de mod\u00e8le tout en rendant inexploitable l'interpr\u00e9tation statistique. \n",
      "- Les impl\u00e9mentations en Python de certains algorithmes dans `scikit-learn` sont souvent plus efficaces et utilisent implicitement les capacit\u00e9s de parall\u00e9lisation.\n",
      "- R offre beaucoup plus de possibilit\u00e9s pour la comparaison de mod\u00e8les statistiques et leur interpr\u00e9tation. \n",
      "\n",
      "En cons\u00e9quences:\n",
      "- Pr\u00e9f\u00e9rer R et ses librairies si la pr\u00e9sentation des r\u00e9sultats et surtout leur interpr\u00e9tation (mod\u00e8les) est prioritaire, si  l'utilisation et / ou la comparaison de beaucoup de m\u00e9thodes est recherch\u00e9e.\n",
      "- Pr\u00e9f\u00e9rer Python et `scikit-learn` pour mettre au point une cha\u00eene de traitements (*pipe line*) op\u00e9rationnelle de l'extraction \u00e0 une analyse privil\u00e9giant la pr\u00e9vision brute \u00e0 l'interpr\u00e9tation et pour des donn\u00e9es quantitatives ou rendues quantitatives (\"vectorisation\" de corpus de textes).\n",
      "\n",
      "En revanche, si les donn\u00e9es sont trop volumineuses pour la taille du disque et distribu\u00e9es sur les n\\oe uds d'un *cluster* avec *Hadoop*, consulter les [calepins](https://github.com/wikistat/Ateliers-Big-Data/tree/master/1-Intro-PySpark) sur l'utilisation de *Spark*.\n",
      "\n",
      "\n",
      "### 1.2 Fonctions d'apprentissage de `Scikit-learn`\n",
      "La communaut\u00e9 qui d\u00e9veloppe cette librairie est tr\u00e8s active et la fait \u00e9voluer rapidement.  Ne pas h\u00e9siter \u00e0 consulter la [documentation](http://scikit-learn.org/stable/user_guide.html) pour des compl\u00e9ments. Voici une s\u00e9lection de ses principales fonctionnalit\u00e9s en lien avec la mod\u00e9lisation.\n",
      "\n",
      "- Transformations (standardisation, discr\u00e9tisation binaire, regroupement de modalit\u00e9s, imputations rudimentaires de donn\u00e9es manquantes) , \"vectorisation\" de corpus de textes (encodage, catalogue, Tf-idf), images;\n",
      "- Mod\u00e9le lin\u00e9aire g\u00e9n\u00e9ral avec p\u00e9nalisation (ridge, lasso, elastic net...), analyse discriminante lin\u00e9aire et quadratique,  $k$ plus proches voisins,  processus gaussiens, classifieur bay\u00e9sien na\u00eff, arbres de r\u00e9gression et classification (CART), agr\u00e9gation de mod\u00e8les (bagging, random forest, adaboost, gradient tree boosting), perceptron multicouche (r\u00e9seau de neurones), SVM (classification, r\u00e9gression, d\u00e9tection d'atypiques...);\n",
      "- Algorithmes de validation crois\u00e9e (loo, k-fold, VC stratifi\u00e9e...) et s\u00e9lection de mod\u00e8les, optimisation sur une grille de param\u00e8tres, s\u00e9paration al\u00e9atoire apprentissage et test, courbe ROC;\n",
      "- Encha\u00eenement (*pipeline*) de traitements.\n",
      "\n",
      "En r\u00e9sum\u00e9, cette librairie est focalis\u00e9e sur les aspects \"machine\" de l'apprentissage de donn\u00e9es quantitatives (s\u00e9ries, signaux, images) volumineuses tandis que R int\u00e8gre l'analyse de variables qualitatives complexes et l'interpr\u00e9tation statistique fine des r\u00e9sultats au d\u00e9triment parfois de l'efficacit\u00e9 des calculs.\n",
      "\n",
      "### 1.3 Objectif\n",
      "L'objectif est d'illustrer la mise en oeuvre de quelques fonctionnalit\u00e9s. Consulter la [documentation](http://scikit-learn.org/stable/user_guide.html) et ses nombreux [exemples](http://scikit-learn.org/stable/auto_examples/index.html) pour plus de d\u00e9tails sur les possibilit\u00e9s d'utilisation de `scikit-learn`. \n",
      "\n",
      "Deux jeux de donn\u00e9es \u00e9l\u00e9mentaires sont utilis\u00e9s. Celui [d\u00e9j\u00e0 \u00e9tudi\u00e9](https://github.com/wikistat/Intro-Python) avec `pandas` et concernant le naufrage du Titanic. Il m\u00e9lange des variables explicatives qualitatives et quantitatives dans un objet de la classe `DataFrame`. Pour \u00eatre utilis\u00e9 dans `scikit-learn` les donn\u00e9es doivent \u00eatre transform\u00e9es en un objet de classe `Array` de `numpy` par le  remplacement des variables qualitatives par les indicatrices de leurs modalit\u00e9s.  L'autre ensemble de donn\u00e9es est enti\u00e8rement quantitatif. C'est un probl\u00e8me classique et simplifi\u00e9 de [reconnaissance de caract\u00e8res](http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits) qui est inclus dans la librairie `scikit-learn`.\n",
      "\n",
      "Apr\u00e8s la phase d'exploration ([calepin pr\u00e9c\u00e9dent](https://github.com/wikistat/Intro-Python)), ce sont les fonctions de mod\u00e9lisation et apprentissage qui sont abord\u00e9es: [r\u00e9gression logistique](http://wikistat.fr/pdf/st-m-app-rlogit.pdf) (titanic), [$k$- plus proches voisins](http://wikistat.fr/pdf/st-m-app-add.pdf) (caract\u00e8res), [arbres de discrimination](http://wikistat.fr/pdf/st-m-app-cart.pdf), et [for\u00eats al\u00e9atoires](http://wikistat.fr/pdf/st-m-app-agreg.pdf). Les param\u00e8tres de complexit\u00e9 des mod\u00e8les  sont optimis\u00e9s par minimisation de l'[erreur de pr\u00e9vision](http://wikistat.fr/pdf/st-m-app-risque-estim.pdf) estim\u00e9e par [validation crois\u00e9e](http://wikistat.fr/pdf/st-m-app-risque-estim.pdf) *V-fold$.  \n",
      "\n",
      "D'autres fonctionnalit\u00e9s sont rapidement illustr\u00e9es : encha\u00eenement (*pipeline*) de m\u00e9thodes et automatisation, d\u00e9tection d'observations atypiques. Leur ma\u00eetrise est n\u00e9anmoins importante pour la mise en exploitation de codes complexes efficaces."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 2 Extraction des \u00e9chantillons\n",
      "Le travail pr\u00e9liminaire consiste \u00e0 s\u00e9parer les \u00e9chantillons en une partie *apprentissage* et une autre de *test* pour estimer sans biais l'[erreur de pr\u00e9vision](http://wikistat.fr/pdf/st-m-app-risque-estim.pdf). L'optimisation (biais-variance) de la complexit\u00e9 des mod\u00e8les est r\u00e9alis\u00e9e en minimisant l'erreur estim\u00e9e par [validation crois\u00e9e](http://wikistat.fr/pdf/st-m-app-risque-estim.pdf) $V-fold$. \n",
      "\n",
      "### 2.1 Donn\u00e9es \"Caract\u00e8res\"\n",
      "Elles sont disponibles dans la librairie `Scikit-learn`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Importations \n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn import datasets\n",
      "%matplotlib inline\n",
      "# les donn\u00e9es\n",
      "digits = datasets.load_digits()\n",
      "# Contenu et mode d'obtention\n",
      "print(digits)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'images': array([[[  0.,   0.,   5., ...,   1.,   0.,   0.],\n",
        "        [  0.,   0.,  13., ...,  15.,   5.,   0.],\n",
        "        [  0.,   3.,  15., ...,  11.,   8.,   0.],\n",
        "        ..., \n",
        "        [  0.,   4.,  11., ...,  12.,   7.,   0.],\n",
        "        [  0.,   2.,  14., ...,  12.,   0.,   0.],\n",
        "        [  0.,   0.,   6., ...,   0.,   0.,   0.]],\n",
        "\n",
        "       [[  0.,   0.,   0., ...,   5.,   0.,   0.],\n",
        "        [  0.,   0.,   0., ...,   9.,   0.,   0.],\n",
        "        [  0.,   0.,   3., ...,   6.,   0.,   0.],\n",
        "        ..., \n",
        "        [  0.,   0.,   1., ...,   6.,   0.,   0.],\n",
        "        [  0.,   0.,   1., ...,   6.,   0.,   0.],\n",
        "        [  0.,   0.,   0., ...,  10.,   0.,   0.]],\n",
        "\n",
        "       [[  0.,   0.,   0., ...,  12.,   0.,   0.],\n",
        "        [  0.,   0.,   3., ...,  14.,   0.,   0.],\n",
        "        [  0.,   0.,   8., ...,  16.,   0.,   0.],\n",
        "        ..., \n",
        "        [  0.,   9.,  16., ...,   0.,   0.,   0.],\n",
        "        [  0.,   3.,  13., ...,  11.,   5.,   0.],\n",
        "        [  0.,   0.,   0., ...,  16.,   9.,   0.]],\n",
        "\n",
        "       ..., \n",
        "       [[  0.,   0.,   1., ...,   1.,   0.,   0.],\n",
        "        [  0.,   0.,  13., ...,   2.,   1.,   0.],\n",
        "        [  0.,   0.,  16., ...,  16.,   5.,   0.],\n",
        "        ..., \n",
        "        [  0.,   0.,  16., ...,  15.,   0.,   0.],\n",
        "        [  0.,   0.,  15., ...,  16.,   0.,   0.],\n",
        "        [  0.,   0.,   2., ...,   6.,   0.,   0.]],\n",
        "\n",
        "       [[  0.,   0.,   2., ...,   0.,   0.,   0.],\n",
        "        [  0.,   0.,  14., ...,  15.,   1.,   0.],\n",
        "        [  0.,   4.,  16., ...,  16.,   7.,   0.],\n",
        "        ..., \n",
        "        [  0.,   0.,   0., ...,  16.,   2.,   0.],\n",
        "        [  0.,   0.,   4., ...,  16.,   2.,   0.],\n",
        "        [  0.,   0.,   5., ...,  12.,   0.,   0.]],\n",
        "\n",
        "       [[  0.,   0.,  10., ...,   1.,   0.,   0.],\n",
        "        [  0.,   2.,  16., ...,   1.,   0.,   0.],\n",
        "        [  0.,   0.,  15., ...,  15.,   0.,   0.],\n",
        "        ..., \n",
        "        [  0.,   4.,  16., ...,  16.,   6.,   0.],\n",
        "        [  0.,   8.,  16., ...,  16.,   8.,   0.],\n",
        "        [  0.,   1.,   8., ...,  12.,   1.,   0.]]]), 'data': array([[  0.,   0.,   5., ...,   0.,   0.,   0.],\n",
        "       [  0.,   0.,   0., ...,  10.,   0.,   0.],\n",
        "       [  0.,   0.,   0., ...,  16.,   9.,   0.],\n",
        "       ..., \n",
        "       [  0.,   0.,   1., ...,   6.,   0.,   0.],\n",
        "       [  0.,   0.,   2., ...,  12.,   0.,   0.],\n",
        "       [  0.,   0.,  10., ...,  12.,   1.,   0.]]), 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'DESCR': \"Optical Recognition of Handwritten Digits Data Set\\n===================================================\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 5620\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttp://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\nReferences\\n----------\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\\n\", 'target': array([0, 1, 2, ..., 8, 9, 8])}\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "images_and_labels = list(zip(digits.images, \n",
      "   digits.target))\n",
      "for index, (image, label) in  enumerate(images_and_labels[:8]):\n",
      "     plt.subplot(2, 4, index + 1)\n",
      "     plt.axis('off')\n",
      "     plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
      "     plt.title('Training: %i' % label)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAWYAAADtCAYAAACMJt+5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEdJJREFUeJzt3X2MXXWdx/HPhwcXSnGmZRddjH2ADUtWdzuBpllCFgYE\noxvWFtYSDSAT2bT+sTElZmn3SQZZs61uNq2BjTWytDbIQtGWaEBTAlOCxk2ozLhoFKUP1LUsD2WG\nZ0X57R/njlybGc5v7txz7/eevl/JpHdmvvec3/3eez9z5sz59eeUkgAAcRzT7QEAAH4XwQwAwRDM\nABAMwQwAwRDMABAMwQwAwfREMNu+1/bV7a4Fva0a/a1OnXvrqq5jtv2SpMmNnyTpNUm/aXy+KqV0\nRyU77iDb75N0i6R3S/pvSUMppSc7sN9a99b28ZLukHSOpIWSLkwp7e7g/uve3z+XdJOks1U8rhFJ\nn0wpPdWBfde9t38i6SuSTldx4PtDSWtTSg/PZDuVHTGnlOamlE5OKZ0s6YCkSyc/b26+7eOqGkOV\nbP++pK9J+kdJ8yQ9IunOTuy77r1teEjSVZKe0ptv5I44CvrbL+mLKn7oLZT0oqTbOrHjo6C3/ytp\npaRTVOTCf0m6e6Yb6fipDNuDtn9u+3rbhyTdarvf9jdtP237sO1v2H5X031GbF/buD1k+2Hbn2/U\n7rX9gRZrF9t+yPYLtnfZvsX2tsyHcrmkx1JKX0sp/UrSsKQlts+cfZdaU5feppReTyl9IaX0Hb15\nNNV1Nervtxqv25dSSq+q+K3vvDa1qSU16u1ESmlfKk5FHCvpDUmHZtqPbp1jfoeKnyYLJK1ujOPW\nxucLJL0q6eam+qTfPWpaJunHKn4qfa5x31Zqvyrpe5LmqwjWq5rva3vM9kemeQzvkTT2252m9Iqk\nn0l673QPukPq0NvI6tjf8yU9lllbpdr01vZ4Y7zXS/rwW9VOKaVU+YekfZIuatwelPRLSW97i/oB\nSYebPn9Q0scbt4ck/bTpe3NU/FQ6dSa1Kp7o1yWd0PT9bZK2ZT6mL0v61yO+9rCkj3Wip3Xu7RHj\nPSjp/E729Cjr759Jek7SefS27b2dI2mDpO+r8fe83I9uHTE/k4pf/yVJtufY3mx7v+0JSbsl9dn2\nNPf/7R8pUnGkKklzZ1h7moon+bWm2oMzeAwvSXr7EV/rU3G+rpvq0NvIatNf238k6V4Vf/j7zkzv\nX4Ha9LZpu+sknSnpT2dy324F85F/zPmUisEvSyn1SbpAkhsfVTkkab7tE5u+tmAG9/+hpCWTn9g+\nSdIZja93Ux16G1kt+mt7oaRdkj6TUrq9nYObhVr09gjHqsjZV8oKm0W5jnmuivMxE7bnS7qh6h2m\nlA6ouJJi2Pbxts+VdKnyrwDYIem9ti+3fYKKMY+mlB6vZsQt68XeyvbvNfoqSc23o+m5/jb+gPaA\npJtTSl+qbqSz1ou9vdj2gO1jbb9d0r9L+klK6WczGUeUI+aNkk6U9Kyk70q6b4qa5vse+b1Wa6+U\ndK6Kc2w3qbjcrflXqcdsf3TKDaf0rKS/lvRZSYclLZUU4Y9ZPd/bhp+oOMo4TdK3Jb1sO8JRdx36\n+zeSFqsInxcbHy9MU9tJdehtv4pr8MdVvIb/QNKHpqmdVmUTTHqR7Tsl/SildGO3x1I39LZa9Lc6\n3ehtlFMZXWF7qe0zbB9j+4MqfrLt7Pa46oDeVov+VidCb3t1dk27vFPS11Vcy3hQ0idSSmNvfRdk\norfVor/V6XpvOZUBAMG044i5Lcm+ffv2rLq1a9eW1lxyySWlNevXry+tmTdvXtaYMrV6iU9Hf3IO\nDg6W1oyPj5fWDA8Pl9asWLEiY0TZWulvR3s7MjJSWpPTk4GBgbbsawa6+trdsGFDac26detKaxYv\nXpy1vz179pTWtDEbpuztUX2OGQAiIpgBIBiCGQCCIZgBIBiCGQCCIZgBIBiCGQCCCTPzL+f6ZEna\nt29fac3zzz9fWjN//vzSmrvuuitrTCtXrsyq6wX9/f2lNbt3l6+L2q5rdnvB6OhoVt2FF15YWtPX\n11das3///qz99YKc649z3oebN28urVm9enXWmHKuY7744ouzttUqjpgBIBiCGQCCIZgBIBiCGQCC\nIZgBIBiCGQCCIZgBIBiCGQCCIZgBIJiOzPzLmUmTM6NPkp544onSmtNPP720JmeVk5xxS70x8y93\ndlq7Vr7IWWWjLnbuzFunc8mSJaU1ObMhb7yxPgthr1q1qrQmZ1bwOeecU1qTu4JJ1bP6cnDEDADB\nEMwAEAzBDADBEMwAEAzBDADBEMwAEAzBDADBEMwAEExHJpjkLPV09tlnZ20rZ/JIjpwL0nvFxo0b\nS2uGh4eztjUxMTHL0RQGBwfbsp1esGbNmqy6RYsWtWVby5cvz9pfL8h5P+/du7e0JmeCWu7EkZy8\nmjdvXta2WsURMwAEQzADQDAEMwAEQzADQDAEMwAEQzADQDAEMwAEQzADQDBhJpjkrCjSThEuIm+X\nnEkJQ0NDWdtq12MeHx9vy3a6Ledx5EzwkfJXOimzZcuWtmynV+RMQjl8+HBpTe4Ek5y6+++/v7Rm\nNu8ljpgBIBiCGQCCIZgBIBiCGQCCIZgBIBiCGQCCIZgBIBiCGQCC6cgEk5wLrffs2dO2/eVMHnnk\nkUdKa6644op2DOeoNDo6WlozMDDQgZHMTs7KL5s2bWrb/nbs2FFa09/f37b91UVOxuRMCpGk1atX\nl9Zs2LChtGb9+vVZ+5sKR8wAEAzBDADBEMwAEAzBDADBEMwAEAzBDADBEMwAEAzBDADBEMwAEExH\nZv7lLA2TMxNPkrZv396Wmhxr165ty3bQu3KW5BoZGcna1tjYWGnNZZddVlqzfPny0prcpcRWrFiR\nVddN69atK63JWQ4qZ0awJO3atau0pupZwRwxA0AwBDMABEMwA0AwBDMABEMwA0AwBDMABEMwA0Aw\nBDMABBNmgknOUi1S3qSPpUuXlta0cymrXpC7HFHO5IV77rmntCZn0kXuJIhuyln+KmcZrdy6nKWs\ncvq/aNGijBH1xgSTnGWjVq1a1bb95Uwe2bx5c9v2NxWOmAEgGIIZAIIhmAEgGIIZAIIhmAEgGIIZ\nAIIhmAEgGIIZAIJxSqnbYwAANOGIGQCCIZgBIBiCGQCCIZgBIBiCGQCCIZgBIBiCGQCCIZgBIBiC\nGQCCIZgBIBiCGQCCIZgBIBiCGQCCIZgBIBiCGQCCIZgBIBiCGQCCIZgBIBiCGQCCIZgBIBiCGQCC\nIZgBIBiCGQCCIZgBIBiCGQCCIZgBIBiCGQCCIZgBIBiCGQCCIZgBIBiCGQCCIZgBIBiCGQCCIZgB\nIJieCGbb99q+ut21oLdVorfVqnN/nVKqZsP2S5ImN36SpNck/abx+aqU0h2V7LgLbH9a0rCki1NK\nD3Rgf7Xure1FkvZKernpy+tTSp/twL5r3VtJsj1H0r9JWinpeEljKaULOrTvWvfX9pWSvtj0pWMk\nnSjpnJTSo7nbOa7dA5uUUpo7edv2PknXThVato9LKf26qnFUzfYZkj4s6Red2ufR0ltJb09VHTlM\n4yjp7ZdUBMZZkg5LGujUjuve35TS7ZJun/zc9jWS/mkmoSx14VSG7UHbP7d9ve1Dkm613W/7m7af\ntn3Y9jdsv6vpPiO2r23cHrL9sO3PN2r32v5Ai7WLbT9k+wXbu2zfYnvbDB/SzZLWSnp9Nn1phxr2\nNsyptrr01vZZkv5KxdHpc6kwo9CoQl36O4UhSV+Z6Z269cJ/h6R5khZIWt0Yx62NzxdIelVF4E1K\nevPXH0laJunHkk6R9LnGfVup/aqk70mar+JUxFXN97U9Zvsj0z0I2yslvZZSuq/k8XZSLXrbcMD2\nQdv/afuUktpOqENvl0k6IOkztp+x/QPbl5c87k6pQ3/VVLdQ0l+ohWBWSqnyD0n7JF3UuD0o6ZeS\n3vYW9QOSDjd9/qCkjzduD0n6adP35kh6Q9KpM6lV8US/LumEpu9vk7Qt8zGdLOlxSQuOfIyd/Khp\nb0+SdLaKN+apkrZL+ha9bUtv/6GxrU+rOJV5vqQXJZ1Ff2ff3yPG+8+SHmilN906Yn4mpfSryU9s\nz7G92fZ+2xOSdkvqs+1p7v/U5I2U0iuNm3NnWHuaiif5tabagzN4DMMqnqwnm7423Xg7qed7m1J6\nOaX0/ZTSGymlpyX9raT32z4pdxsV6fneqjjqfF3Sv6SUfp1SekhFaL1/BtuoSh362+xjkra2csdu\nBfORf9D5lKQzJS1LKfVJukBFyFUZdIckzbd9YtPXFszg/hdJ+qTtQ41zYu+WdJftv2vnIFtQh95O\np9vnnOvQ2x80/j1yjB39I+s06tBfSZLt8yT9oaS7WxlEt1/ok+aq+Ek+YXu+pBuq3mFK6YCkRyQN\n2z7e9rmSLlX+C/R9kt4jaYmKX7F+IWmVpP+oYLiz0XO9tb3M9h/bPqZxbvkLkh5MKb1Y3ahb0nO9\nVXHU+aSkv7d9XCNABiV9u4rxzlIv9nfSNZLuTim9XFo5hShHzBtVXOv3rKTvSrpviprm+x75vVZr\nr5R0rqTnJN0k6U5Jzb9KPWb7o1NuOKXDKaWnGx//p+JazOdbfSLaqOd7K+n0xjhfkPQ/Kt6c09V2\nUs/3NhWXoC2X9JeSxiVtlnR1SunxacbSST3f38b3T1BxjXhLpzGkCieY9CLbd0r6UUrpxm6PpW7o\nbXXobbW60d8opzK6wvZS22c0fmX+oKQPSdrZ7XHVAb2tDr2tVoT+Vjbzr0e8U9LXVVzLeFDSJ1JK\nY90dUm3Q2+rQ22p1vb+cygCAYNpxxNzRZB8fHy+tGRoaKq3ZubPjv/m1eolPW/o7ODiYVbdo0aLS\nmi1btsxqLBVppb8dfe3mPAc5r+/R0dE2jGZGuvra3bhxY2lNTt9y3/NjY+UHx319faU1+/fvL63p\n7++fsrdH9TlmAIiIYAaAYAhmAAiGYAaAYAhmAAiGYAaAYAhmAAim52b+5VxDOzDQsSXMekbONZWS\ntHv37tKarVvL/2+WhQsXltbkjim63Otjc3p7ww2V/wdqtdTf319ak3M9dG5dznXTOWOaDkfMABAM\nwQwAwRDMABAMwQwAwRDMABAMwQwAwRDMABAMwQwAwRDMABBMmJl/OTNppLyZf2vWrCmtaeess5xV\nP7otdxbSgQMHSmtyVm9o12od0uxmUHXC8PBw27a1YsWKtm2rLnLezzlyn6ecbBgZGZnVWMpwxAwA\nwRDMABAMwQwAwRDMABAMwQwAwRDMABAMwQwAwRDMABBMmAkmORNHpLyLv4eGhkprci5az53Y0M4J\nBlXJnQQzNjZWWjMxMVFak7O8V/SJI7lyJ8osWbKktOZoWxYtZ6JGuyZz5C4tlSNnObGcHJoOR8wA\nEAzBDADBEMwAEAzBDADBEMwAEAzBDADBEMwAEAzBDADBdGSCSc7F2Nddd13Wtq655prZDkeStGnT\nptKa2267rS37iiDnOZDyLuYfHR0trcl9PnO0awWLquROMMmZ5JMzCSJnlZNeWFVHyhtnzuutnSuK\n5LxXclbomQ2OmAEgGIIZAIIhmAEgGIIZAIIhmAEgGIIZAIIhmAEgGIIZAILpyASTnJUq+vr6sra1\ndevW0pqcC9Jz5FzIXzdVXzjfLGc1ml6QO5lj9+7dpTU5k1VyJu88+uijWWPq9oopOb3LmfBhu7Rm\nx44dOUPq6HtgOhwxA0AwBDMABEMwA0AwBDMABEMwA0AwBDMABEMwA0AwBDMABEMwA0AwHZn5lzOT\nJnd5npxZfTn7y1miKmfGYq/IXVoq5zEPDw/PcjSFusysHBoayqrLmbGXMxMuZ8Zk7vPd7Zl/OXKW\nFsuZORxhRl8ujpgBIBiCGQCCIZgBIBiCGQCCIZgBIBiCGQCCIZgBIBiCGQCC6cgEk3bKmQAxMTFR\nWpM7KaAuRkZGsuo2bdrUlv3lTODppQv+30ruaylnYsiWLVtKa3L6VpfJO1Leazenb700YYwjZgAI\nhmAGgGAIZgAIhmAGgGAIZgAIhmAGgGAIZgAIhmAGgGCcUur2GAAATThiBoBgCGYACIZgBoBgCGYA\nCIZgBoBgCGYACIZgBoBgCGYACIZgBoBgCGYACIZgBoBgCGYACIZgBoBgCGYACIZgBoBgCGYACIZg\nBoBgCGYACIZgBoBgCGYACOb/AZQMEvFS/SJnAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x9f9a410>"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# variables pr\u00e9dictives et cible\n",
      "X=digits.data\n",
      "y=digits.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from sklearn.model_selection import train_test_split\n",
      "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=11)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2.2 Donn\u00e9es \"Titanic\"\n",
      "\n",
      "Les donn\u00e9es sur le naufrage du Titanic sont d\u00e9crites dans le calepin consacr\u00e9 \u00e0 la librairie *pandas*. Reconstruire la table des donn\u00e9es en lisant le fichier .csv."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Lire les donn\u00e9es d'apprentissage\n",
      "import pandas as pd\n",
      "#path=''  # si les donn\u00e9es sont d\u00e9j\u00e0 dans le r\u00e9pertoire courant\n",
      "path='http://www.math.univ-toulouse.fr/~besse/Wikistat/data/'\n",
      "df=pd.read_csv(path+'titanic-train.csv',skiprows=1,header=None,usecols=[1,2,4,5,9,11],\n",
      "  names=[\"Surv\",\"Classe\",\"Genre\",\"Age\",\"Prix\",\"Port\"],dtype={\"Surv\":object,\"Classe\":object,\"Genre\":object,\"Port\":object})\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Surv</th>\n",
        "      <th>Classe</th>\n",
        "      <th>Genre</th>\n",
        "      <th>Age</th>\n",
        "      <th>Prix</th>\n",
        "      <th>Port</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>0</td>\n",
        "      <td>3</td>\n",
        "      <td>male</td>\n",
        "      <td>22</td>\n",
        "      <td>7.2500</td>\n",
        "      <td>S</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>1</td>\n",
        "      <td>1</td>\n",
        "      <td>female</td>\n",
        "      <td>38</td>\n",
        "      <td>71.2833</td>\n",
        "      <td>C</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>1</td>\n",
        "      <td>3</td>\n",
        "      <td>female</td>\n",
        "      <td>26</td>\n",
        "      <td>7.9250</td>\n",
        "      <td>S</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>1</td>\n",
        "      <td>1</td>\n",
        "      <td>female</td>\n",
        "      <td>35</td>\n",
        "      <td>53.1000</td>\n",
        "      <td>S</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>0</td>\n",
        "      <td>3</td>\n",
        "      <td>male</td>\n",
        "      <td>35</td>\n",
        "      <td>8.0500</td>\n",
        "      <td>S</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "  Surv Classe   Genre  Age     Prix Port\n",
        "0    0      3    male   22   7.2500    S\n",
        "1    1      1  female   38  71.2833    C\n",
        "2    1      3  female   26   7.9250    S\n",
        "3    1      1  female   35  53.1000    S\n",
        "4    0      3    male   35   8.0500    S"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.shape # dimensions\n",
      "# Red\u00e9finir les types \n",
      "df[\"Surv\"]=pd.Categorical(df[\"Surv\"],ordered=False)\n",
      "df[\"Classe\"]=pd.Categorical(df[\"Classe\"],ordered=False)\n",
      "df[\"Genre\"]=pd.Categorical(df[\"Genre\"],ordered=False)\n",
      "df[\"Port\"]=pd.Categorical(df[\"Port\"],ordered=False)\n",
      "df.dtypes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "Surv      category\n",
        "Classe    category\n",
        "Genre     category\n",
        "Age        float64\n",
        "Prix       float64\n",
        "Port      category\n",
        "dtype: object"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "Surv      891\n",
        "Classe    891\n",
        "Genre     891\n",
        "Age       714\n",
        "Prix      891\n",
        "Port      889\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# imputation des valeurs manquantes\n",
      "df[\"Age\"]=df[\"Age\"].fillna(df[\"Age\"].median())\n",
      "df.Port=df[\"Port\"].fillna(\"S\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Discr\u00e9tiser les variables quantitatives\n",
      "df[\"AgeQ\"]=pd.qcut(df.Age,3,labels=[\"Ag1\",\"Ag2\",\"Ag3\"])\n",
      "df[\"PrixQ\"]=pd.qcut(df.Prix,3,labels=[\"Pr1\",\"Pr2\",\"Pr3\"])\n",
      "# red\u00e9finir les noms des modalit\u00e9s \n",
      "df[\"Surv\"]=df[\"Surv\"].cat.rename_categories([\"Vnon\",\"Voui\"])\n",
      "df[\"Classe\"]=df[\"Classe\"].cat.rename_categories([\"Cl1\",\"Cl2\",\"Cl3\"])\n",
      "df[\"Genre\"]=df[\"Genre\"].cat.rename_categories([\"Gfem\",\"Gmas\"])\n",
      "df[\"Port\"]=df[\"Port\"].cat.rename_categories([\"Pc\",\"Pq\",\"Ps\"])\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Il est n\u00e9cessaire de transformer les donn\u00e9es car `scikit-learn` ne reconna\u00eet pas la classe `DataFrame` de `pandas`, ce qui est bien dommage. Les variables qualitatives sont comme pr\u00e9c\u00e9demment remplac\u00e9es par les indicatrices de leurs modalit\u00e9s et les variables quantitatives conserv\u00e9es. Cela introduit une \u00e9vidente redondance dans les donn\u00e9es mais les proc\u00e9dures de s\u00e9lection de mod\u00e8le feront le tri."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Table de d\u00e9part\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Construction des indicatrices\n",
      "df_q=df.drop([\"Age\",\"Prix\"],axis=1)\n",
      "df_q.head()\n",
      "# Indicatrices\n",
      "dc=pd.DataFrame(pd.get_dummies(df_q[[\"Surv\",\"Classe\",\"Genre\",\"Port\",\"AgeQ\",\"PrixQ\"]]))\n",
      "dc.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Table des indicatrices\n",
      "df1=pd.get_dummies(df_q[[\"Surv\",\"Classe\",\"Genre\",\"Port\",\"AgeQ\",\"PrixQ\"]])\n",
      "# Une seule indicatrice par variable binaire\n",
      "df1=df1.drop([\"Surv_Vnon\",\"Genre_Gmas\"],axis=1)\n",
      "# Variables quantitatives\n",
      "df2=df[[\"Age\",\"Prix\"]]\n",
      "# Concat\u00e9nation\n",
      "df_c=pd.concat([df1,df2],axis=1)\n",
      "# V\u00e9rification\n",
      "df_c.columns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Extraction des \u00e9chantillons d'apprentissage et test."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# variables explicatives\n",
      "T=df_c.drop([\"Surv_Voui\"],axis=1)\n",
      "# Variable \u00e0 mod\u00e9liser\n",
      "z=df_c[\"Surv_Voui\"]\n",
      "# Extractions\n",
      "from sklearn.model_selection import train_test_split\n",
      "T_train,T_test,z_train,z_test=train_test_split(T,z,test_size=0.2,random_state=11)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Attention**: l'\u00e9chantillon test des donn\u00e9es \"Titanic\" est relativement petit, l'estimation de l'erreur de pr\u00e9vision est donc sujette \u00e0 caution car probablement de grande variance. Il suffit de changer l'initialisation (param\u00e8tre ` random_state`) et r\u00e9-ex\u00e9cuter les scripts pour s'en assurer. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 3 *K* plus proches voisins\n",
      "Les images des caract\u00e8res sont cod\u00e9es par des variables  quantitatives. Le probl\u00e8me de reconnaissance de forme ou de discrimination est adapt\u00e9 \u00e0 l'algorithme des  [$k$-plus proches voisins](http://wikistat.fr/pdf/st-m-app-add.pdf). Le param\u00e8tre \u00e0 optimiser pour contr\u00f4ler la complexit\u00e9 du mod\u00e8le est le nombre de voisin `n_neighbors`. Les autres options sont d\u00e9crites dans la [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "knn = KNeighborsClassifier(n_neighbors=10)\n",
      "digit_knn=knn.fit(X_train, y_train) \n",
      "# Estimation de l'erreur de pr\u00e9vision\n",
      "# sur l'\u00e9chantillon test\n",
      "1-digit_knn.score(X_test,y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Optimisation du param\u00e8tre de complexit\u00e9 du mod\u00e8le par validation crois\u00e9e en cherchant l'erreur minimale sur une grille de valeurs du param\u00e8tre avec `cv=5`-*fold cross validation* et `n_jobs=-1` pour une ex\u00e9cution en parall\u00e8le utilisant tous les processeurs sauf 1. Attention, comme la validation crois\u00e9e est al\u00e9atoire, deux ex\u00e9cutions successives ne donnent pas le m\u00eame r\u00e9sultat."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.model_selection import GridSearchCV\n",
      "# grille de valeurs\n",
      "param=[{\"n_neighbors\":list(range(1,15))}]\n",
      "knn= GridSearchCV(KNeighborsClassifier(),param,cv=5,n_jobs=-1)\n",
      "digit_knnOpt=knn.fit(X_train, y_train)\n",
      "# param\u00e8tre optimal\n",
      "digit_knnOpt.best_params_[\"n_neighbors\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Le mod\u00e8le `digit_knnOpt` est d\u00e9j\u00e0 estim\u00e9 avec la valeur \"optimale\" du param\u00e8tre."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Estimation de l'erreur de pr\u00e9vision sur l'\u00e9chantillon test\n",
      "1-digit_knnOpt.score(X_test,y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Pr\u00e9vision\n",
      "y_chap = digit_knnOpt.predict(X_test)\n",
      "# matrice de confusion\n",
      "table=pd.crosstab(y_test,y_chap)\n",
      "print(table)\n",
      "plt.matshow(table)\n",
      "plt.title(\"Matrice de Confusion\")\n",
      "plt.colorbar()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3.3 R\u00e9gression logistique\n",
      "La pr\u00e9vision de la survie, variable binaire des donn\u00e9es \"Titanic\", se pr\u00eatent \u00e0 une [r\u00e9gression logistique](http://wikistat.fr/pdf/st-m-app-rlogit.pdf). Les versions p\u00e9nalis\u00e9es (ridge, lasso, elastic net, lars) du mod\u00e8le lin\u00e9aire g\u00e9n\u00e9ral sont les algorithmes les plus d\u00e9velopp\u00e9s dans `Scikit-learn` au d\u00e9triment de ceux plus classiques (*forward, backward, step-wise*) de s\u00e9lection de variables en optimisant un crit\u00e8re de type AIC.  Une version lasso de la r\u00e9gression logistique est test\u00e9e afin d'introduire la s\u00e9lection automatique des variables.\n",
      "\n",
      "Estimation et erreur de pr\u00e9vision du mod\u00e8le complet sur l'\u00e9chantillon test."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "logit = LogisticRegression()\n",
      "titan_logit=logit.fit(T_train, z_train)\n",
      "# Erreur sur l'\u00e9cahntillon test\n",
      "1-titan_logit.score(T_test, z_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Coefficients\n",
      "titan_logit.coef_ "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Comme pour le mod\u00e8le lin\u00e9aire, il faudrait construire les commandes d'aide \u00e0 l'interpr\u00e9tation des r\u00e9sultats.\n",
      "\n",
      "P\u00e9nalisation et optimisation du param\u00e8tre par validation crois\u00e9e. Il existe une fonction sp\u00e9cifique mais son mode d'emploi est peu document\u00e9; `GridSearchCV` lui est pr\u00e9f\u00e9r\u00e9e."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# grille de valeurs\n",
      "param=[{\"C\":[0.01,0.096,0.098,0.1,0.12,1,10]}]\n",
      "logit = GridSearchCV(LogisticRegression(penalty=\"l1\"),\n",
      "   param,cv=5,n_jobs=-1)\n",
      "titan_logitOpt=logit.fit(T_train, z_train)\n",
      "# param\u00e8tre optimal\n",
      "titan_logitOpt.best_params_[\"C\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Estimation de l'erreur de pr\u00e9vision par le mod\u00e8le \"optimal\"."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Erreur sur l'\u00e9chantillon test\n",
      "1-titan_logitOpt.score(T_test, z_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Petit souci suppl\u00e9mentaire, l'objet produit par `GridSearchCV` ne conna\u00eet pas l'attribut `.coef_`. Il faut donc r\u00e9-estimer le mod\u00e8le pour conna\u00eetre les coefficients."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Estimation avec le param\u00e8tre optimal et coefficients\n",
      "LogisticRegression(penalty=\"l1\",C=titan_logitOpt.best_params_['C']).fit(T_train, z_train).coef_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Commenter : parcimonie du mod\u00e8le vs. erreur de pr\u00e9vision."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 4 Arbre de d\u00e9cision\n",
      "### 4.1 Impl\u00e9mentation\n",
      "Les [arbres binaires de d\u00e9cision](http://wikistat.fr/pdf/st-m-app-cart.pdf) (CART: *classification and regression trees*) s'appliquent \u00e0 tous types de variables. Les options de l'algorithme sont d\u00e9crites dans la [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html). La complexit\u00e9 du mod\u00e8le est g\u00e9r\u00e9e par deux param\u00e8tres : `max_depth`, qui d\u00e9termine le nombre max de feuilles dans l'arbre, et le nombre minimales `min_samples_split` d'observations requises pour rechercher une dichotomie. \n",
      "\n",
      "**Attention**: M\u00eame s'il s'agit d'une impl\u00e9mentation proche de celle originale propos\u00e9e par Breiman et al. (1984) il n'existe pas (encore?) comme dans R (package `rpart`) un param\u00e8tre de p\u00e9nalisation de la d\u00e9viance du mod\u00e8le par sa complexit\u00e9 (nombre de feuilles) afin de construire une s\u00e9quence d'arbres embo\u00eet\u00e9s dans la perspective d'un \u00e9lagage (*pruning*) optimal par validation crois\u00e9e. La fonction g\u00e9n\u00e9rique de $k$-*fold cross validation* `GridSearchCV` est utilis\u00e9e pour optimiser le param\u00e8tre de profondeur mais sans beaucoup de pr\u00e9cision dans l'\u00e9lagage car ce dernier \u00e9limine tout un niveau et pas les seules feuilles inutiles \u00e0 la qualit\u00e9 de la pr\u00e9vision.\n",
      "\n",
      "En revanche, l'impl\u00e9mentation anticipe sur celles des [m\u00e9thodes d'agr\u00e9gation de mod\u00e8les](http://wikistat.fr/pdf/st-m-app-agreg.pdf) en int\u00e9grant les param\u00e8tres (nombre de variables tir\u00e9es, importance...) qui leurs sont sp\u00e9cifiques. D'autre part, la repr\u00e9sentation graphique d'un arbre n'est pas incluse et n\u00e9cessite l'impl\u00e9mentation d'un autre logiciel libre: [Graphviz](http://www.graphviz.org/). \n",
      "\n",
      "Tout ceci souligne encore les objectifs de d\u00e9veloppement de cette librairie: temps de calcul et pr\u00e9vision brute au d\u00e9triment d'une recherche d'interpr\u00e9tation. Dans certains exemples \u00e9ventuellement pas trop compliqu\u00e9s, un arbre \u00e9lagu\u00e9 de fa\u00e7on optimal peut en effet pr\u00e9voir \u00e0 peine moins bien (diff\u00e9rence non significative) qu'une agr\u00e9gation de mod\u00e8les (for\u00eat al\u00e9atoire ou **boosting**) et apporter un \u00e9clairage nettement plus pertinent qu'un algorithme de type \"bo\u00eete noire\". \n",
      "\n",
      "## 4.2 Donn\u00e9es \"Titanic\"\n",
      "Estimation de l'arbre complet."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.tree import DecisionTreeClassifier\n",
      "tree=DecisionTreeClassifier()\n",
      "digit_tree=tree.fit(T_train, z_train) \n",
      "# Estimation de l'erreur de pr\u00e9vision\n",
      "1-digit_tree.score(T_test,z_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Optimisation du param\u00e8tre de complexit\u00e9 du mod\u00e8le par validation crois\u00e9e en cherchant l'erreur minimale sur une grille de valeurs du param\u00e8tre avec `cv=5`-*fold cross validation* et `n_jobs=-1` pour une ex\u00e9cution en parall\u00e8le utilisant tous les processeurs sauf 1. Attention, comme la validation crois\u00e9e est al\u00e9atoire et un arbre un mod\u00e8le instable, deux ex\u00e9cutions successives ne donnent pas n\u00e9cessairement le m\u00eame r\u00e9sultat."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "param=[{\"max_depth\":list(range(2,10))}]\n",
      "titan_tree= GridSearchCV(DecisionTreeClassifier(),param,cv=5,n_jobs=-1)\n",
      "titan_opt=titan_tree.fit(T_train, z_train)\n",
      "# param\u00e8tre optimal\n",
      "titan_opt.best_params_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "La valeur \"optimale\" du param\u00e8tre reste trop importante pour la lisibilit\u00e9 de l'arbre. Une valeur plus faible est utilis\u00e9e."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tree=DecisionTreeClassifier(max_depth=3)\n",
      "titan_tree=tree.fit(T_train, z_train)\n",
      "# Estimation de l'erreur de pr\u00e9vision\n",
      "# sur l'\u00e9chantillon test\n",
      "1-titan_tree.score(T_test,z_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Noter l'am\u00e9lioration de l'erreur."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# pr\u00e9vision de l'\u00e9chantillon test\n",
      "z_chap = titan_tree.predict(T_test)\n",
      "# matrice de confusion\n",
      "table=pd.crosstab(z_test,z_chap)\n",
      "print(table)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Tracer l'arbre avec le logiciel Graphviz."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.tree import export_graphviz\n",
      "from sklearn.externals.six import StringIO  \n",
      "import pydot\n",
      "dot_data = StringIO() \n",
      "export_graphviz(titan_tree, out_file=dot_data) \n",
      "graph=pydot.graph_from_dot_data(dot_data.getvalue()) \n",
      "graph.write_png(\"titan_tree.png\")  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "L'arbre est g\u00e9n\u00e9r\u00e9 dans un fichier image \u00e0 visualiser pour se rende compte qu'il est plut\u00f4t mal \u00e9lagu\u00e9 et pas directement interpr\u00e9table sans les noms en clair des variables et modalit\u00e9s."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import Image\n",
      "Image(filename='titan_tree.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 4.3 Donn\u00e9es   \"Caract\u00e8res\"\n",
      "La m\u00eame d\u00e9marche est utilis\u00e9e pour ces donn\u00e9es."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Arbre complet\n",
      "tree=DecisionTreeClassifier()\n",
      "digit_tree=tree.fit(X_train, y_train) \n",
      "# Estimation de l'erreur de pr\u00e9vision\n",
      "1-digit_tree.score(X_test,y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Optimisation par validation crois\u00e9e\n",
      "param=[{\"max_depth\":list(range(5,15))}]\n",
      "digit_tree= GridSearchCV(DecisionTreeClassifier(),param,cv=5,n_jobs=-1)\n",
      "digit_treeOpt=digit_tree.fit(X_train, y_train)\n",
      "digit_treeOpt.best_params_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Estimation de l'erreur de pr\u00e9vision\n",
      "1-digit_treeOpt.score(X_test,y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Echantillon test\n",
      "y_chap = digit_treeOpt.predict(X_test)\n",
      "# matrice de confusion\n",
      "table=pd.crosstab(y_test,y_chap)\n",
      "print(table)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.matshow(table)\n",
      "plt.title(\"Matrice de Confusion\")\n",
      "plt.colorbar()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Comme pour les autres m\u00e9thodes, l'objet `GridSearchCV` ne contient pas tous les attibuts, dont celui `tree`, et ne permet pas de construire l'arbre. Il faudrait le r\u00e9-estimer mais comme il est bien trop complexe, ce r\u00e9sultat n'est pas produit."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 5 For\u00eats al\u00e9atoires\n",
      "L'algorithme d'agr\u00e9gation de mod\u00e8les le plus utilis\u00e9 est celui des [for\u00eats al\u00e9atoires](http://wikistat.fr/pdf/st-m-app-agreg.pdf) (random forest) de Breiman (2001) ce qui ne signifie pas qu'il conduit toujours \u00e0 la meilleure pr\u00e9vision. Voir la [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) pour la signification de tous les param\u00e8tres.\n",
      "\n",
      "Plus que le nombre d'arbres `n_estimators`, le param\u00e8tre \u00e0 optimiser est le nombre de variables tir\u00e9es al\u00e9atoirement pour la recherche de la division optimale d'un noeud: `max_features`. Par d\u00e9faut, il prend la valeur $\\frac{p}{3}$ en  r\u00e9gression et $\\sqrt{p}$ en discrimination.\n",
      "### 5.1 Donn\u00e9es \"Caract\u00e8res\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier \n",
      "# d\u00e9finition des param\u00e8tres\n",
      "forest = RandomForestClassifier(n_estimators=500, \n",
      "   criterion='gini', max_depth=None,\n",
      "   min_samples_split=2, min_samples_leaf=1, \n",
      "   max_features='auto', max_leaf_nodes=None,\n",
      "   bootstrap=True, oob_score=True)\n",
      "# apprentissage et erreur out-of-bag\n",
      "forest = forest.fit(X_train,y_train)\n",
      "print(1-forest.oob_score_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# erreur de pr\u00e9vision sur le test\n",
      "1-forest.score(X_test,y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "L'optimisation du param\u00e8tre `max_features` peut \u00eatre r\u00e9alis\u00e9e en minimisant l'erreur de pr\u00e9vision *out-of-bag*. Ce n'est pas pr\u00e9vu, il est aussi possible comme pr\u00e9c\u00e9demment de minimiser l'erreur par validation crois\u00e9e."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "param=[{\"max_features\":list(range(4,64,4))}]\n",
      "digit_rf= GridSearchCV(RandomForestClassifier(n_estimators=100),param,cv=5,n_jobs=-1)\n",
      "digit_rfOpt=digit_rf.fit(X_train, y_train)\n",
      "# param\u00e8tre optimal\n",
      "digit_rfOpt.best_params_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Comme pour les autres m\u00e9thodes, l'objet `GridSearchCV` ne propose pas tous les attributs et donc pas d'erreur *out-of-bag* ou d'importance des variables. Voir le tutoriel sur la [pr\u00e9vision du pic d'ozone](https://github.com/wikistat/Apprentissage/tree/master/Pic-ozone) pour plus de d\u00e9tails."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# erreur de pr\u00e9vision sur le test\n",
      "1-digit_rfOpt.score(X_test,y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# pr\u00e9vision\n",
      "y_chap = digit_rfOpt.predict(X_test)\n",
      "# matrice de confusion\n",
      "table=pd.crosstab(y_test,y_chap)\n",
      "print(table)\n",
      "plt.matshow(table)\n",
      "plt.title(\"Matrice de Confusion\")\n",
      "plt.colorbar()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 5.2 Donn\u00e9es \"Titanic\"\n",
      "M\u00eame d\u00e9marche."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# d\u00e9finition des param\u00e8tres\n",
      "forest = RandomForestClassifier(n_estimators=500, criterion='gini', max_depth=None, \n",
      "    min_samples_split=2, min_samples_leaf=1, max_features='auto', max_leaf_nodes=None,bootstrap=True, oob_score=True)\n",
      "# apprentissage\n",
      "forest = forest.fit(T_train,z_train)\n",
      "print(1-forest.oob_score_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# erreur de pr\u00e9vision sur le test\n",
      "1-forest.score(T_test,z_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# optimisation de max_features\n",
      "param=[{\"max_features\":list(range(2,15))}]\n",
      "titan_rf= GridSearchCV(RandomForestClassifier(n_estimators=100),param,cv=5,n_jobs=-1)\n",
      "titan_rfOpt=titan_rf.fit(T_train, z_train)\n",
      "# param\u00e8tre optimal\n",
      "titan_rfOpt.best_params_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# erreur de pr\u00e9vision sur le test\n",
      "1-titan_rfOpt.score(T_test,z_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# pr\u00e9vision\n",
      "z_chap = titan_rfOpt.predict(T_test)\n",
      "# matrice de confusion\n",
      "table=pd.crosstab(z_test,z_chap)\n",
      "print(table)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Modifier la valeur du param\u00e8tre pour constater sa faible influence sur la qualit\u00e9 plut\u00f4t m\u00e9diocre du r\u00e9sultat. \n",
      "\n",
      "**Attention**, comme d\u00e9j\u00e0 signal\u00e9, l'\u00e9chantillon test est de relativement faible taille (autour de 180), il serait opportun d'it\u00e9rer l'extraction al\u00e9atoire d'\u00e9chantillons tests (validation crois\u00e9e *Monte Carlo*)  pour tenter de r\u00e9duire la variance de cette estimation et avoir une id\u00e9e de sa distribution.\n",
      "\n",
      "C'est fait dans d'autres calepins du [d\u00e9p\u00f4t d'apprentissage](https://github.com/wikistat/Apprentissage)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 6 Fonction *pipeline*\n",
      "Pour encha\u00eener et brancher (*plugin*) plusieurs traitements, g\u00e9n\u00e9ralement des transformations suivies d'une mod\u00e9lisation. Utiliser les fonctionnalit\u00e9s de cette section sans mod\u00e9ration afin d'optimiser la structure et l'efficacit\u00e9 (parall\u00e9lisation) de codes complexes. \n",
      "\n",
      "### 6.1 Familles de transformations (*transformers*)\n",
      "Classification ou r\u00e9gression sont souvent la derni\u00e8re \u00e9tape d'un proc\u00e9d\u00e9 long et complexe. Dans la \"vraie vie\", les donn\u00e9es ont besoin d'\u00eatre extraites, s\u00e9lectionn\u00e9es, nettoy\u00e9es, standardis\u00e9es, compl\u00e9t\u00e9es... (*data munging*) avant d'alimenter un algorithme d'apprentissage. Pour structurer le code, *Sciki-learn* propose d'utiliser le principe d'une API (*application programming interface*) nomm\u00e9e *transformer*. \n",
      "\n",
      "Ces fonctionnalit\u00e9s sont illustr\u00e9es sur les m\u00eames donn\u00e9es de reconnaissance de caract\u00e8res. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Rechargement des donn\u00e9es\n",
      "from sklearn.datasets import load_digits\n",
      "from sklearn.model_selection import train_test_split\n",
      "digits = load_digits()\n",
      "X, y = digits.data, digits.target\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
      "\n",
      "# Plot\n",
      "sample_id = 42\n",
      "plt.imshow(X[sample_id].reshape((8, 8)), interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
      "plt.title(\"y = %d\" % y[sample_id])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Normalisations, r\u00e9ductions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "tf = StandardScaler()\n",
      "tf.fit(X_train, y_train)\n",
      "Xt_train = tf.transform(X)  \n",
      "print(\"Moyenne avant centrage et r\u00e9duction =\", np.mean(X_train))\n",
      "print(\"Moyenne apr\u00e8s centrage et r\u00e9duction =\", np.mean(Xt_train))\n",
      "# See also Binarizer, MinMaxScaler, Normalizer, ..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Raccourci: Xt = tf.fit_transform(X)\n",
      "tf.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# NB. La standardisation pr\u00e9alable est indispensable pour certains algorithmes\n",
      "# notamment les SVM\n",
      "from sklearn.svm import SVC\n",
      "clf = SVC()\n",
      "# Calcul des scores (bien class\u00e9s)\n",
      "print(\"Sans standardisation =\", clf.fit(X_train, y_train).score(X_test, y_test))\n",
      "print(\"Avec standardisation =\", clf.fit(tf.transform(X_train), y_train).score(tf.transform(X_test), y_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### S\u00e9lection de variables par \u00e9limination pas \u00e0 pas\n",
      "La proic\u00e9dure `RFE` (*r\u00e9cursive feature selection*) supprime une \u00e0 une les variables les moins significatives ou moins importantes au sens du crit\u00e8re du mod\u00e8le utilis\u00e9; dans cet exemple, il s'agit des for\u00eats al\u00e9atoires."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# S\u00e9lection de variables par \u00e9l\u00e9mination pas \u00e0 pas\n",
      "from sklearn.feature_selection import RFE\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "tf = RFE(RandomForestClassifier(), n_features_to_select=10, verbose=1)\n",
      "Xt = tf.fit_transform(X_train, y_train)\n",
      "print(\"Shape =\", Xt.shape)\n",
      "\n",
      "# Variables (pixels) s\u00e9lectionn\u00e9es\n",
      "plt.imshow(tf.get_support().reshape((8, 8)), interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### D\u00e9composition, factorisation, r\u00e9duction de dimension\n",
      "Possibilit\u00e9, par exemple, de r\u00e9cup\u00e9rer les *q* premi\u00e8res composantes principales de l'ACP comme r\u00e9sultat d'une transformation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# par ACP ou SVD\n",
      "from sklearn.decomposition import PCA\n",
      "tf = PCA(n_components=2)\n",
      "Xt_train = tf.fit_transform(X_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####  Fonction  de transformation d\u00e9finie par l'utilisateur\n",
      "Une fonction de transformation ou *transformer* est d\u00e9finie et s'applique \u00e0 un jeu de donn\u00e9es avec la syntaxe ci-dessous."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.preprocessing import FunctionTransformer\n",
      "def increment(X):\n",
      "    return X + 1\n",
      "tf = FunctionTransformer(func=increment)\n",
      "Xt = tf.fit_transform(X)\n",
      "print(X[0])\n",
      "print(Xt[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 6.4 *Pipelines*\n",
      "\n",
      "Des transformations sont cha\u00een\u00e9es en une s\u00e9quence constituant un *pipeline*."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.pipeline import make_pipeline\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.feature_selection import RFE\n",
      "#tf = RFE(RandomForestClassifier(), n_features_to_select=10)\n",
      "# La succession de deux transformeurs constituent un transformeur\n",
      "tf = make_pipeline(StandardScaler(), RFE(RandomForestClassifier(),n_features_to_select=10))\n",
      "tf.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Xt_train = tf.transform(X_train)\n",
      "print(\"Mean =\", np.mean(Xt_train))\n",
      "print(\"Shape =\", Xt_train.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Une cha\u00eene de transformations suivi d'un classifieur construisent un nouveau classifieur"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = make_pipeline(StandardScaler(), \n",
      "                    RFE(RandomForestClassifier(), n_features_to_select=10), \n",
      "                    RandomForestClassifier())\n",
      "clf.fit(X_train, y_train)\n",
      "print(clf.predict_proba(X_test)[:5])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# L'hyperparam\u00e8tre est accessible\n",
      "print(\"n_features =\", clf.get_params()[\"rfe__estimator__n_estimators\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "L'optimisation des param\u00e8tres par validation crois\u00e9e est obtenue avec la m\u00eame fonction mais peut prendre  du temps si plusieurs param\u00e8tres sont cocern\u00e9s! Le pipeline construit \u00e0 titre illustratif  n'est certainement pas optimal."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grid = GridSearchCV(clf, param_grid={\"rfe__estimator__n_estimators\": [5, 10],\n",
      "                    \"randomforestclassifier__max_features\": [0.1, 0.25, 0.5]})\n",
      "grid.fit(X_train, y_train)\n",
      "print(\"Valeurs optimales =\", grid.best_params_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 6.5 Union de caract\u00e9ristiques\n",
      "\n",
      "Des transformations sont appliqu\u00e9es en parall\u00e8le pour r\u00e9unir en un seul ensemble des transformations des donn\u00e9es."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.pipeline import make_union\n",
      "from sklearn.decomposition import PCA, FastICA\n",
      "tf = make_union(PCA(n_components=10), FastICA(n_components=10))\n",
      "Xt_train = tf.fit_transform(X_train)\n",
      "print(\"Shape =\", Xt_train.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 6.6 Compositions embo\u00eet\u00e9es\n",
      "\n",
      "Comme des  pipelines and des unions sont eux-m\u00eames des estimateurs, ils peuvent \u00eatre compos\u00e9s dans une structure embo\u00eet\u00e9e pour construire des combinaisons complexes de mod\u00e8les comme ceux remportant les concours de type [*kaggle](https://www.kaggle.com/).\n",
      "\n",
      "Les donn\u00e9es initiales sont unies aux composantes de l'ACP, puis les variables les plus importantes au sens des for\u00eats al\u00e9atoires sont s\u00e9lectionn\u00e9es avant de servir \u00e0 l'apprentissage d'un r\u00e9seau de neurones. Ce n'est s\u00fbrement pas une strat\u00e9gie optimale !"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "\n",
      "clf = make_pipeline(\n",
      "    # Build features\n",
      "    make_union(\n",
      "        FunctionTransformer(func=lambda X: X), PCA(),), \n",
      "    # Select the best features\n",
      "    RFE(RandomForestClassifier(), n_features_to_select=10),\n",
      "    # Train\n",
      "    MLPClassifier(max_iter=500)\n",
      ")\n",
      "\n",
      "clf.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Effectivement la combinaison n'est pas optimale:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# erreur de test\n",
      "1-clf.score(X_test,y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    }
   ],
   "metadata": {}
  }
 ]
}